{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Multi-Layer Perceptron\n",
    "\n",
    "In this project we will build out a Multi Layer Perceptron model in Tensorflow to try to classify hand written digits using the famous MNIST data set. The purpose of this project isn't to achieve peak accuracy on the MNIST data set, but rather to demonstrate the underlying concepts of Tensorflow without using contrib.learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "We will be using the famous MNIST data set of [handwritten digits](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "The images which we will be working with are black and white images of size 28 x 28 pixels, or 784 pixels total. Our features will be the pixel values for each pixel. Either the pixel is \"white\" (blank with a 0), or there is some pixel value. \n",
    "\n",
    "We will try to correctly predict what number is written down based solely on the image data in the form of an array.\n",
    "\n",
    "Let's get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in a vector format, although the original data was a 2-dimensional matirx with values representing how dark a pixel is at a given location. We can take a look at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a sample image and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put this back into a shape that can be depicted as an image. Let's reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = mnist.train.images[2].reshape(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import matplotlib.pyplot to view the data as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d44bb1a630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCD+QFOEHkiL8QFKEH0iK8ANJEX4gqf8HAIELZI9qhowAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d44b4ed0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to define a few parameters. The parameters here are:\n",
    "\n",
    "* Learning Rate - How quickly to adjust the cost function.\n",
    "* Training Epochs - How many training cycles to go through\n",
    "* Batch Size - Size of the 'batches' of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to set parameters which will directly define our Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set placeholders for the x and y values we'll be defining soon to use with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build the model. We'll use 2 hidden layers, and a RELU activation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights,biases):\n",
    "    '''\n",
    "    x : Place Holder for Data Input\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dicitionary of biases\n",
    "    '''\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    #weights and biases will be defined later\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our weights and biases dicts. We'll use tf's built-in random_normal method to create the random values for our weights and biases. We need these values to be tf.Variable to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    #outputs random weights at a normal distro\n",
    "    #creating a matrix of these values by n_input rows and n_hidden_1\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h1': <tf.Variable 'Variable:0' shape=(784, 256) dtype=float32_ref>,\n",
       " 'h2': <tf.Variable 'Variable_1:0' shape=(256, 256) dtype=float32_ref>,\n",
       " 'out': <tf.Variable 'Variable_2:0' shape=(256, 10) dtype=float32_ref>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': <tf.Variable 'Variable_3:0' shape=(256,) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'Variable_4:0' shape=(256,) dtype=float32_ref>,\n",
       " 'out': <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the model with the function and values we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to define the cost and optimazation variables for this model to be evaluated. We'll use Tensorflow's built-in functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-edc7c611a607>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize all those tf.Variable objects we created earlier. This will be the first thing we run when training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model. We'll be using next_batch for this. Let's look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsamp,ysamp = mnist.train.next_batch(1)\n",
    "ysamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d44c300b00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8ERfiBoP4ftO9p7iYAhaMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d44bbe19b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xsamp.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(ysamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the onehot functionality at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=159.3263\n",
      "Epoch: 2 cost=41.6960\n",
      "Epoch: 3 cost=25.6383\n",
      "Epoch: 4 cost=18.0160\n",
      "Epoch: 5 cost=13.0839\n",
      "Epoch: 6 cost=9.7305\n",
      "Epoch: 7 cost=7.2755\n",
      "Epoch: 8 cost=5.3702\n",
      "Epoch: 9 cost=4.1645\n",
      "Epoch: 10 cost=3.1113\n",
      "Epoch: 11 cost=2.2881\n",
      "Epoch: 12 cost=1.6629\n",
      "Epoch: 13 cost=1.3251\n",
      "Epoch: 14 cost=1.1551\n",
      "Epoch: 15 cost=0.7359\n",
      "Model has completed 15 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intialize all the variables\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    # Convert total number of batches to integer\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Grab the next batch of training data and labels\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Feed dictionary for optimization and loss value\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow comes with some built-in functions to help evaluate our model, including tf.equal and tf.cast with tf.reduce_mean. Let's use these to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a numerical value for our predictions we will need to use tf.cast to cast the Tensor of booleans back into a Tensor of Floating point values in order to take the mean of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the tf.reduce_mean function in order to grab the mean of the elements across the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still a tensor, and we still need to pass our test data and labels through. We can call the MNIST test labels and images and evaluate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the built-in eval() method to compare these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9409\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94% isn't too bad, but this actually isn't anywhere near as good as it could be. Again, the purpose of this project isn't to achieve peak accuracy on the MNIST data set, but rather to demonstrate the underlying concepts of Tensorflow without using contrib.learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
